---
title: "Untitled"
author: "Gesa Amelung"
date: "7 November 2017"
output:
  pdf_document: default
  html_document: default
bibliography: rpc_lit.bib
editor_options: 
  chunk_output_type: console
  
---

# Introduction

Changes in the communty of diatoms in response to several environmental factors have been well studied. Thereby, changes in diatom community structure  can be taken as a proxy for environmental pollution with e.g. nutrients. However, the response of the diatom community to pesticides has not been well studied so far. Especially the response to herbicides is of special concern since they are the type of herbicides that are most toxic to diatoms. 

In this study, monitoring data from lotic waterbodies in Germany from the years 2005 - 2012 is used to perform Correspondance Analysis (CA). With ordination via CA, the main trend of variation can be visualized. With subsequent Canonical Correspondance Analysis (CCA), this trend can be connected to environmental descriptors. Those environmental descriptors are represented by a dataset of physical-chemical data and herbicide data sampled in lotic waterbodies of Germany as well.

# Data Analysis

## Setup

The following packages and the data base access script was loaded.

```{r setup, echo=TRUE, message=FALSE}
## load packages
require(RPostgreSQL)
require(sf)
require(knitr)
require(mapview)
require(data.table)
require(dplyr)
require(vegan)
require(ggplot2)
require(gridExtra)
require(stringr)

## switch
online = FALSE ##AS: added switch (see data loading chunks below)

## load access data
path = getwd() ##AS: necessary to make it run for me

#path = 'D:/Gesa/Dokumente/Ecotoxicology/RPC'
#path = '//SAMBASERVER/home/amel7631/Desktop/pesticides-organisms'
source(file.path(path, 'amelung_access.R'))
```

## Loading data

### Pesticide data

The pesticide data was loaded including the samples, the sites and geographic information of the sites.

```{r load-PSM-data, echo=TRUE, eval=FALSE, message=FALSE}
if (online) {
  drv = dbDriver("PostgreSQL")
  con = dbConnect(drv, dbname = DBname, user = DBuser, host = DBhost, port = DBport, password = DBpassword)
  
  psm_sites_geo = st_read_db(con, query = "SELECT *
                                           FROM views.psm_sites_2005")
  psm_sites = as.data.frame(psm_sites_geo)
  psm_sam = dbGetQuery(con, "SELECT *
                                 FROM views.psm_samples_2005 sam
                                 JOIN phch.phch_variables var
                                ON var.variable_id = sam.variable_id")
  psm_sam = psm_sam[ ,-c(16, 24)] # delete duplicate rows
  
  dbDisconnect(con)
  dbUnloadDriver(drv)
  
  saveRDS(psm_sites_geo, file.path('cache', 'psm_sites_geo.rds'))
  saveRDS(psm_sites, file.path('cache', 'psm_sites.rds'))
  saveRDS(psm_sam, file.path('cache', 'psm_sam.rds'))
} else {
  psm_sites_geo <- readRDS(file.path('cache', 'psm_sites_geo.rds'))
  psm_sites <- readRDS(file.path('cache', 'psm_sites.rds'))
  psm_sam <- readRDS(file.path('cache', 'psm_sam.rds'))
}

psm_sam = psm_sam %>%
  filter(psm_type == 'herbicide' & date > '2004-12-31')
## AS: when the script is finished we could also consider applying it to all pesticides

# Create column for year
psm_sam$year <- format(psm_sam$date, '%Y')
```

### Diatom data
```{r load-diatom-data, echo=TRUE, message=FALSE}
if (online) {
  drv = dbDriver("PostgreSQL")
  con = dbConnect(drv, dbname = DBname, user = DBuser, host = DBhost, port = DBport, password = DBpassword)
  # dia_sites = dbGetQuery(con, "SELECT * FROM dia.dia_sites") ##AS: no need to extra load this
  dia_sites_geo = st_read_db(con, query = "SELECT * FROM dia.dia_sites")
  dia_sites = as.data.frame(dia_sites_geo)
  dia_sam = dbGetQuery(con, "SELECT *
                         FROM dia.dia_samples
                         WHERE date > '2004-12-31'
                         ")
  dbDisconnect(con) ##AS: close beore doing the belwo changes
  dbUnloadDriver(drv)
  
  saveRDS(dia_sites_geo, file.path('cache', 'dia_sites_geo.rds'))
  saveRDS(dia_sites, file.path('cache', 'dia_sites.rds'))
  saveRDS(dia_sam, file.path('cache', 'dia_sam.rds'))

} else {
  
  dia_sites_geo = readRDS(file.path('cache', 'dia_sites_geo.rds'))
  dia_sites = readRDS(file.path('cache', 'dia_sites.rds'))
  dia_sam = readRDS(file.path('cache', 'dia_sam.rds'))
}

# Create column for species name only. Add "sp." to those names that only have genera 
dia_sam$species <- gsub('[[:punct:]]', '', dia_sam$taxon) # remove punctuation characters
dia_sam$species <- gsub('([a-z]+)\\s([a-z]+)(.+)*', '\\1 \\2', dia_sam$species) # reduce to two strings (removes: 'var', 'form')
dia_sam$species <- ifelse(str_count(dia_sam$species, pattern = '\\s') == 0,
                          paste0(dia_sam$species, ' sp.'),
                          dia_sam$species)

# table(str_count(dia_sam$species, pattern = '\\s')) # count space characters (only 1s should be left) ##AS: tests for two name taxa

# Create column for genus name
dia_sam$genus <- sub("^(\\S*).*", "\\1", dia_sam$taxon)

# Create column for year
dia_sam$year <- format(dia_sam$date, '%Y')
dia_sam$month <- format(dia_sam$date, '%m')
dia_sam$day <- format(dia_sam$date, '%j') ##AS: Added day of the year
```

### River network
```{r load-river-network}
if (online) {
  drv = dbDriver("PostgreSQL")
  con = dbConnect(drv, dbname = DBname, user = DBuser, host = DBhost, port = DBport, password = DBpassword)
  
  streams = st_read_db(con, query = "SELECT * FROM spatial.gewaessertyp")
  
  dbDisconnect(con)
  dbUnloadDriver(drv)
  
  saveRDS(streams, file.path('cache', 'streams.rds'))

} else {
  
  streams = readRDS(file.path('cache', 'streams.rds'))
}
```

### Load physical-chemical data
```{r load-phch-data, echo=TRUE, message=FALSE}
if (online) {
  drv = dbDriver("PostgreSQL")
  con = dbConnect(drv, dbname = DBname, user = DBuser, host = DBhost, port = DBport, password = DBpassword)

  phch_sam = dbGetQuery(con, "SELECT *
                              FROM phch.phch_samples sam
                              LEFT JOIN phch.phch_variables var ON var.variable_id = sam.variable_id
                                  WHERE date > '2004-12-31' AND date < '2016-01-01'")
  dbDisconnect(con)
  dbUnloadDriver(drv)
  
  saveRDS(phch_sam, file.path('cache', 'phch_sam.rds'))
} else {
  phch_sam = readRDS(file.path('cache', 'phch_sam.rds'))
}

phch_sam = phch_sam[ , -c(16, 24)] #AS! removes duplicated unit, variable_id entries

# Get only data that are not psm or psm = herbicide
phch_sam <- phch_sam[phch_sam$psm_type%in%c("herbicide", ""),]

phch_sam$year <- format(phch_sam$date, '%Y')  # Create column for year
phch_sam$month <- format(phch_sam$date, '%m') # Create column for month
phch_sam$day <- format(phch_sam$date, '%j')   # Create column for day of the year

```

## Data preparation

For the CCA later on, a dataset was created containing only diatom samples where herbicides were measured at the same site in the same year (`dia_sam_cca`). For the CA, all samples were retained in order to have a larger dataset  (`dia_sam`).

```{r join-on-site-year, echo=TRUE, message=FALSE}
# Diatom data

dia_sam_cca <- semi_join(dia_sam, phch_sam[phch_sam$psm==TRUE,], by = c("site_id", "year"))
nrow(dia_sam_cca)

##AS: like this you can keep it an sf-object:
##AS: generally you can handle sf-Objects with dplyr like data.frames and tibbles
#dia_sites_cca <- dia_sites_geo[ dia_sites_geo$site_id %in% psm_sites_geo$site_id, ]
```

All  physical-chemical data were deleted that were not taken at a diatom sampling site.
```{r join-phch, echo=TRUE, message=FALSE}
phch_sam <- semi_join(phch_sam, dia_sam_cca, by ="site_id")
length(unique(phch_sam$site_id)) 
```

### Usage of cover data as quantitative variable
In most cases, diatom cover was measured instead of abundance. Therefore, rows where abundance was measured were deleted. Instead, the cover data is used as the quantitative variable in the following analyses. The deletion of irrelevant rows was performed for the datasets for CA and CCA seperately.

```{r delete-abundance-data, echo=TRUE, message=FALSE}
dia_sam <- dia_sam[!is.na(dia_sam$cover), ]
dia_sam <- dia_sam[,-6]    #delete column with abundance data since its mostly emtpy

dia_sam_cca <- dia_sam_cca[!is.na(dia_sam_cca$cover), ]
dia_sam_cca <- dia_sam_cca[,-6] 
```

## Taxon frequency
[@Marchant.2002] suggests to remove rare species as only common species produce information that can reliably be linked to present environmental gradients. [@Cao.2001] suggests the removal of rare species if samples were taken over a large spatial scale which holds true for the present dataset. Furthermore, Chi-square distance, which is the distance measure used in CA and CCA, gives more weight to rare species. Due to this reason, [@Legendre.2012] suggest to remove rare species from the analysis. Here, species that occur at less than 20 % of the sites were remove. 

The analysis was done for both, species and genus data and therefore, all procedures were performed for species data and genus data seperately from here on. According to (@Rimet.2012), analysis with a taxonomic resolution of genus performs as well as a resolution of species level in many cases. Similarly, (@Larras.2014) found that diatom sensitivity to herbicides can be attributed to their phylogeny. They reported that centric and araphid diatoms (Thalassiosirales and Fragilariales, respectively
) were most sensitive to the herbicides whereas pennates (Cymbellales, Naviculales and Bacillariales)
) were more robust.

```{r  taxon-frequency, echo=TRUE, message=FALSE}
##AS: with this approach you can remove species which occur in less than x% of the sites
# e.g. Fernandez 2016 used only species which occurred at 20% of the sites
site_idN = length(unique(dia_sam$site_id)) # count of unique sites
sp_n <- length(unique(dia_sam$species))
sp_n_ge <- length(unique(dia_sam$genus))

# Rare genera
dia_sam_ge = dia_sam %>%
  group_by(genus) %>%
  mutate(perc = n_distinct(site_id) / site_idN) %>%
  arrange(-perc) %>%
  filter(perc >= 0.2)

# precentage of genera remaining after removal of rare genera
sp_n_ge_abu <-length(unique(dia_sam_ge$genus)) #32
sp_n_ge_abu/sp_n_ge*100 # 40.5 %

# Plot
dia_sam_N_ge = dia_sam_ge %>%
       group_by(genus) %>%
       summarise(N=n())
plot(density(dia_sam_N_ge$N), main="Genus density")

# Rare species

dia_sam = dia_sam %>%
  group_by(species) %>%
  mutate(perc = n_distinct(site_id) / site_idN) %>%  # not aggregating
  # summarise(perc = n_distinct(site_id) / site_idN) %>%
  arrange(-perc) %>%
  filter(perc >= 0.2)
##AS: Is it better to do this with genus or species? Let's consider both

# precentage of species remaining after removal of rare species
sp_n_abundant <- length(unique(dia_sam$species)) # 78
sp_n_abundant/sp_n*100 #8.6 %

# Plot
dia_sam_N = dia_sam %>%
       group_by(species) %>%
       summarise(N=n())
plot(density(dia_sam_N$N), main="Species density")

## Remove rare species
#dia1 = dia_cov %>%
#  group_by(species) %>%
#  summarise(N = n()) %>%
#  arrange(-N) %>%
#  filter(N > 1)

#plot(density(dia1$N), main="species density")

#spec_list = sort(dia1$species)
#length(spec_list)
# 614 different diatom species

#dia_cov2 <- dia_cov[ !dia_cov$species %in% spec_list, ]
#nrow(dia_cov2)

## Genus frequency
#dia_gen = dia_cov2 %>%
#  group_by(genus) %>%
#  summarise(N = n()) %>%
#  arrange(-N) 

#gen_list = sort(dia_gen$genus)
#length(gen_list)

#plot(density(dia_gen$N), main="genus density")

```
The distribution of species frequency is very left-skewed. Most species are present in less than 500 times after removal of rare species.

## Visualization of the sampling frequency

Sampling days and sampling events per year.

```{r sampling-days-per-year, echo=TRUE, message=FALSE}
## Number of samples

# Count number of samples per month and year
sample_count_ym = dia_sam %>%
  group_by(year, month) %>%
  summarise(N = n()) %>%
   as.data.frame()

# Count number of samples per year
sample_count_y = dia_sam %>%
  group_by(year) %>%
  summarise(N = n()) %>%
   as.data.frame()

## Number of sampling events

# Count number of sampling events (id_pn) per year
event_count_y <- dia_sam %>% 
  group_by(year) %>%
  summarise(N = n_distinct(id_pn))  %>%
   as.data.frame()

# Count number of sampling events per month and year
event_count_ym <- dia_sam %>% 
  group_by(year, month) %>%
  summarise(N = n_distinct(id_pn))  %>%
   as.data.frame()

## Plots

# Plot: Number of samples per month and year
ggplot(sample_count_ym, aes(y = N, x = month)) +
  geom_point() +
  facet_wrap( ~ year) +
  ggtitle ("Number of samples per month and year")

# Plot: Number of sampling events per month and year
ggplot(event_count_ym, aes(y = N, x = month)) +
  geom_point() +
  facet_wrap( ~ year) +
  ggtitle ("Number of sampling events per month and year")

## Plot: Sampling counts per year & Event counts per year
gg1 <- ggplot(dia_cov, aes(x=year)) +
  geom_bar() +
  labs(title="Sample counts per year", x = "year", y = "sample counts")

gg2 <- ggplot(event_count_y, aes(x=year, y = N)) +
  geom_bar(stat="identity") +
  labs(title="Event counts per year", x = "year", y = "event counts")

grid.arrange(gg1, gg2, ncol=2)

gg3 <- ggplot(dia_sam, aes(x=month)) +
  geom_bar() +
  labs(title="Samples per month", x = "month", y = "number of samples")
gg3

```

It seems like the distribution of samples and sampling events is very similar. This means that on every sampling day and event, a similar number of samples was taken. For the years 2013 and 2014, only few data are available, they should therefore be removed before any further analyses. Also, if aggregation is to be done per month over all years, March, April and November should be removed since there are only few data available.

## Transform data to get from long to wide data

For the subsequent analysis, the data needed to be transformend from long to wide format. If several combinations of (`site_id`)  and (`species`) were present, the 95th percentile was taken to aggregate them. Further, the first column was stored as rowname. The aggregation was first performed over all years and over all months seperately.

### Aggregation per year

```{r  transformation-95th, echo=TRUE, message=FALSE}
# Aggreagate over all years
dia_aggr <- dia_sam %>%
  group_by(species, site_id) %>%
  summarise(mean = mean(cover),
            perc95 = quantile(cover, 0.95, na.rm = TRUE),
            total = n()) %>%
  arrange(species)

# aggregate for all years seperately
dia_aggr_year <- dia_sam %>% 
  group_by(year, species, site_id) %>%
  summarise(mean = mean(cover),
            perc95 = quantile(cover, 0.95, na.rm = TRUE),
            total = n()) %>%
  arrange(species)

# Store single years as seperate elements of a list
dia_aggr_split <- split(dia_aggr_year, dia_aggr_year$year)

cast_years <- lapply(dia_aggr_split,
                     function(x) dcast(x, site_id ~ species,
                                       value.var="perc95", fill = 0))

cast_years <- lapply(cast_years,
                     function(x) { rownames(x) <- x[,1];
                                   x[,1] <- NULL;
                                   x })

# Take the first column and store it as a rowname, afterwards, delete the first column.
cast_years <- lapply(cast_years,
       function(x) { x[rowSums(x) > 0,] })

```
 
#### graphical inspection of years
```{r ggplot-year}
# overview plots

length(unique(dia_cov$site_id)) ##AS: 712
length(unique(dia_sam$site_id)) ##AS: 1194

gr1 = ggplot(dia_sam, aes(y = site_id, x = month)) +
  geom_raster(aes(fill = year)) +
  facet_wrap( ~ year)
gr1

ggsave(gr1, width = 8, height = 5, filename = '/tmp/dia_sam.png')

gr2 = ggplot(dia_cov, aes(y = site_id, x = year)) +
  geom_raster(aes(fill = log10(cover+1)))
gr2
##AS: Maybe skip 2013/14?

gr3 = ggplot(dia_cov) +
  geom_raster(aes(y = site_id, x = day, fill = year))
gr3
```

```{r  delete-2013-2014, echo=TRUE, message=FALSE}
cast_years[["2013"]] <- NULL
cast_years[["2014"]] <- NULL
```

#### DCA

A Detrended Correspondance Analysis (DCA) was performed in order to decide whether it is appropriate to conduct a unimodal gradient method. DCA was introduced by (@Hill.1980). If the length of the first DCA axis, scaled in units of standard deviations (S.D.), is > 4 S.D. a complete species turnover can be assumed and a unimodal gradient method is appropriate to use. A linear gradient method would be more appropriate for a length of the first axes < 3 S.D., indicating an incomplete change of species composition. For an axis length between 3 and 4, both types of methods are appropriate to use (@Leps.2003).

```{r  dca, echo=TRUE, message=FALSE}
lapply(cast_years, function(x) {decorana(x)})
lapply(cast_years, function(x) {plot(decorana(x), display="both")})
```

The length of the first axis is > 3 S.D. in all years and therefore, the unimodal gradient method Correspondance Analysis (CA) can be used for further analyses.

### Aggregation per month

The data were also aggregated per month over all years by the 95th percentile of the cover. The data were then transformed from long to wide format. The months March, April and November were excluded from further analysis since for these months, only few data were available.

Diatom communities follow a seasonal succession which might be more prominent than the interanual succession. (@Hausmann.2007) showed that for lakes, the spring and summer conditions had a strong impact on the diatom composition and productivity that was stronger than the impact of changing annual mean temperatures. Similarly,  (@Kirilova.2008) observed a seasonal succession of diatoms in  lakes. The predominant diatoms in winter and spring were of the genus *Stephanodiscus* while species of the genera *Stephanodiscus*, *Fragilaria*, and *Nitzschia* were the most abundant diatoms in summer and autumn. To account for this seasonal variability of diatom composition, it seemed reasonable to summarize the diatom data for each month over all years.

 
##### Species              
```{r cast-month, echo=TRUE, message=FALSE}
dia_aggr_month <- dia_sam %>% 
  group_by(month, species, site_id) %>%
  summarise(mean = mean(cover),
            perc95 = quantile(cover, 0.95, na.rm = TRUE),
            total = n()) %>%
  arrange(species)

# Store single years as seperate elements of a list
dia_aggr_split_m <- split(dia_aggr_month, dia_aggr_month$month)

cast_months <- lapply(dia_aggr_split_m, function(x) dcast(x, site_id ~ species, value.var="perc95", fill = 0))

# Take the first column and store it as a rowname, afterwards, delete the first column.
cast_months <- lapply(cast_months,
                     function(x) { rownames(x) <- x[,1];
                                   x[,1] <- NULL;
                                   x })

# Remove rows where the row sum is zero.
cast_months <- lapply(cast_months,
       function(x) { x[rowSums(x) > 0,] })

# names(cast_months)
# Remove the months March and November since there is not enough data.
cast_months[["03"]] <- NULL
cast_months[["04"]] <- NULL
cast_months[["11"]] <- NULL
```

#### Genus
```{r cast-month-ge, echo=TRUE, message=FALSE}
dia_aggr_month_ge <- dia_sam_ge %>% 
  group_by(month, genus, site_id) %>%
  summarise(mean = mean(cover),
            perc95 = quantile(cover, 0.95, na.rm = TRUE),
            total = n()) %>%
  arrange(genus)

# Store single years as seperate elements of a list
dia_aggr_split_m_ge <- split(dia_aggr_month_ge, dia_aggr_month_ge$month)

cast_months_ge <- lapply(dia_aggr_split_m_ge, function(x) dcast(x, site_id ~ genus, value.var="perc95", fill = 0))

# Take the first column and store it as a rowname, afterwards, delete the first column.
cast_months_ge <- lapply(cast_months_ge,
                     function(x) { rownames(x) <- x[,1];
                                   x[,1] <- NULL;
                                   x })

# Remove rows where the row sum is zero.
cast_months_ge <- lapply(cast_months_ge,
       function(x) { x[rowSums(x) > 0,] })

# names(cast_months)
# Remove the months March and November since there is not enough data.
cast_months_ge[["03"]] <- NULL
cast_months_ge[["04"]] <- NULL
cast_months_ge[["11"]] <- NULL
```

#### DCA per month

Is the length of the first axis long enough to indicate complete species turnover?

##### Species

```{r dca-month, echo=TRUE, message=FALSE}
lapply(cast_months, function(x) {decorana(x)})
lapply(cast_months, function(x) {plot(decorana(x), display="both")})
```
The legth of the first axis is > 3 S.D. in all months and therefore, a unimodal gradient can be assumed for all months.

##### Genus
```{r dca-month-ge, echo=TRUE, message=FALSE}
lapply(cast_months_ge, function(x) {decorana(x)})
lapply(cast_months_ge, function(x) {plot(decorana(x), display="both")})
```

Also for genus data, the length of the irst axis is > 3 S.D. in all months and therefore, a unimodal gradient can be assumed for all months.

## CA

Correspondance Analysis (CA) was used to determine the major trends of variation of the diatom community before including any environmental data. CA is an ordination method and hence reduces data of multidimensional space to a 2- or 3-dimensional representation with minimal loss of information. The dimensions included in the ordination plot are the ones with the highest amount of variance and thus display the greatest part of variability of the data. The advantage of CA is that, by using weighted averages ordination, sites and species are ordered at the same time into the same space. In the ordination plot, similar communities and species are close together while dissimilar communities and species are further apart.

This was done seperately per year and per month.

### CA per year
```{r  ca-year, echo=TRUE, message=FALSE}
# conduct CA
ca_y <-lapply(cast_years, function(x) {cca(x)})
lapply(ca_y, function(x) {summary(x)})
```

## Ordination plots
```{r plot-ca-year, echo=TRUE, message=FALSE}
dianame_y <- lapply(cast_years, function(x) make.cepnames(names(x)))
stems_y <- lapply(cast_years, colSums)

for (i in 1:length(dianame_y)) {
  plot(ca_y[[i]], dis="sp", type="n") #red crosses = species, dots = sites
  orditorp(ca_y[[i]], dis="sp", lab=dianame_y[[i]], priority=stems_y[[i]],
           pcol="darkgrey", pch="+")
}
```

### CA per month

#### Species

```{r  ca-month, echo=TRUE, message=FALSE}
# conduct CA
ca_m <-lapply(cast_months, function(x) {cca(x)})

# Compute summary of CA results to inspect proportion of variance explained by each axis.
lapply(ca_m, function(x) {summary(x)})
```

```{r  plot-ca-month, echo=TRUE, message=FALSE}
# Compute biplots without names
lapply(cast_months, function(x) {plot(cca(x), scaling=2)})

# Biplots with species names
dianame_long_m <- lapply(cast_months, function(x) colnames(x))

dianame_m <- lapply(cast_months, function(x) make.cepnames(names(x)))
stems_m <- lapply(cast_months, colSums)

for (i in 1:length(dianame_m)) {
  plot(ca_m[[i]], dis="sp", type="n") #red crosses = species, dots = sites
  orditorp(ca_m[[i]], dis="sp", lab=dianame_m[[i]], priority=stems_m[[i]],
           pcol="red", pch="+")
}
```
It seems like there are not one or two main factors explaining the variance in diatom composition but many since the fraction of cumulative explained variation is also not very high when considering several axes.

#### Genus

```{r  ca-month-ge, echo=TRUE, message=FALSE}
# conduct CA
ca_m_ge <-lapply(cast_months_ge, function(x) {cca(x)})

# Compute summary of CA results to inspect proportion of variance explained by each axis.
lapply(ca_m_ge, function(x) {summary(x)})
```

The cumulated explained variance of the first two axes is above 20 % in all months which is a much higher value than when taking species for the CA. Therefore, it might make more sense to proceed analysis with genus data instead of with species data.

```{r  plot-ca-month-ge, echo=TRUE, message=FALSE}
# Compute biplots without names
lapply(cast_months_ge, function(x) {plot(cca(x), scaling=2)})

# Biplots with species names
dianame_long_m_ge <- lapply(cast_months_ge, function(x) colnames(x))

# dianame_m_ge <- lapply(cast_months_ge, function(x) make.cepnames(names(x)))
stems_m_ge <- lapply(cast_months_ge, colSums)

for (i in 1:length(dianame_long_m_ge)) {
  plot(ca_m_ge[[i]], dis="sp", type="n") #red crosses = species, dots = sites
  orditorp(ca_m_ge[[i]], dis="sp", lab=dianame_long_m_ge[[i]], priority=stems_m_ge[[i]],
           pcol="red", pch="+")
}
```


## Environmental data
```{r get-variable-names, echo=TRUE, message=FALSE}
##AS: HAb ich oben schon unter load physical-chemical reingeladen
##as: würde ich heir löschen
drv = dbDriver("PostgreSQL")
con = dbConnect(drv, dbname = DBname, user = DBuser, host = DBhost, port = DBport, password = DBpassword)


var_names = dbGetQuery(con, "SELECT DISTINCT var.name
                                FROM phch.phch_samples sam
                                JOIN phch.phch_variables var ON var.variable_id = sam.variable_id
                                ")
# Nitrit, Nitrat-Stickstoff, Kalium, Sichttiefe, Chlorid, Nitrat, Gesamt-Phosphor, Sauerstoffgehalt, Sauerstoffzehrung nach 5 (7) Tagen ohne Hemmer, Gesamt-Stickstoff, Stickstoff, Wasserstand (OFW), pH-Wert (Feld) Ammonium-Stickstoff, Sauerstoffzehrung nach 7 Tagen mit Hemmer, BSB5, sedimentiert, Sauerstoffzehrung nach 21 Tagen mit Hemmer, TrÃ¼bung (physiko-chem. Messung), Elektrische LeitfÃ¤higkeit (20Â°C), Nitrit-Stickstoff, Natrium, Sulfat, Ammonium, Abfluss, spektraler Absorptionskoeffizient 436nm, freie KohlensÃ¤ure, SauerstoffsÃ¤ttigung, Stickstoff, Hydrogenkarbonat, Eisen, Wassertemperatur, Magnesium, Gesamtphosphor als PO4, Phosphor, Chemischer Sauerstoffbedarf

dbDisconnect(con)
dbUnloadDriver(drv)
```

Combine herbicide data and physical chemical data to form environmental data set.


```{r envi-data}
names(phch_sam)

```




```{r select-envi-variables, echo=TRUE, message=FALSE}
##AS: Eigentlich braucht es den Schritt nicht da alle PSM-Daten sowies auch in phch_sam drinnen sind.
envi <- bind_rows(psm = psm_sam, phch = phch_sam, .id="id") %>%
  select(site_id, date, value_fin, unit, name, psm) 

envi <- envi %>%
  filter(psm == 'TRUE' | name %in% c("Nitit", "Nitrat-Stickstoff", "Kalium", "Chlorid", "Nitrat", "Gesamt-Phosphor", "Sauerstoffgehalt", "Gesamt-Stickstoff", "Stickstoff", "pH-Wert (Feld)", "Ammonium-Stickstoff", "BSB5", "Nitrit-Stickstoff", "Natrium", "Sulfat", "Ammonium", "Abfluss", "Stickstoff", "Hydrogenkarbonat", "Eisen", "Wassertemperatur", "Magnesium", "Phosphor", "Chemischer Sauerstoffgehalt"))

# 6,768,652 observations
envi <- envi[! envi$value_fin =="0",]   # if value is zero, then limit of quantification was not reached
nrow(envi)
# 481,178 observations

# Create column for year
envi$year <- format(envi$date, '%Y')

# Change order of columns
envi = envi[ ,c(1:2,7, 3:6)]

##########################################
# Convert water temperature from celsius to kelvin
#envi <- envi %>%
#  ifelse(name %in% "Wassertemperatur", value_fin + 273.15, value_fin)

#envi_K <- envi %>%
#  filter(grep("Wassertemperatur", name)) %>%
#  mutate(value_fin2 = value_fin + 273.15)

#envi_K <- envi %>%
#  mutate(value_fin2 = ifelse(grepl("Wassertemperatur", name), value_fin + 273.15, #ifelse(grepl(!"Wassertemperatur", name), value_fin)))

# none of these work yet..

```

Aggregate data over all years and within years. Then, transform data from long to wide format.
```{r  transformation-95th-envi, echo=TRUE, message=FALSE}

# Aggreagate over all years
# setDF(dia_cov)
envi_aggr <- envi %>%
  group_by(name, site_id) %>%
  summarise(mean = mean(value_fin),
            perc95 = quantile(value_fin, 0.95, na.rm = TRUE),
            total = n()) %>%
  arrange(name)

# Transform data into wide format
envi_cast <- dcast(envi_aggr, site_id ~ name, value.var ="perc95", fill=0)

# Make site_id column into rowname
rownames(envi_cast) <- (envi_cast[,1])
envi_cast <- envi_cast[,-1]

# aggregate for all years seperately
envi_aggr_year <- envi %>%
  group_by(year, name, site_id) %>%
  summarise(mean = mean(value_fin),
            perc95 = quantile(value_fin, 0.95, na.rm = TRUE),
            total = n()) %>%
  arrange(name)

# Store single years as seperate elements of a list
envi_aggr_split <- split(envi_aggr_year, envi_aggr_year$year)
envi_aggr_split[["2005"]] # check out structure

# Transform data from long to wide format
envi_cast_years <- lapply(envi_aggr_split, function(x) dcast(x, site_id ~ name, value.var="perc95", fill = 0))
names(envi_cast_years)
envi_cast_years[["2014"]] # checkt out structure

# Change rownames using the first column
rownames(envi_cast_years[["2005"]]) <- (envi_cast_years[["2005"]][,1])
rownames(envi_cast_years[["2006"]]) <- (envi_cast_years[["2006"]][,1])
rownames(envi_cast_years[["2007"]]) <- (envi_cast_years[["2007"]][,1])
rownames(envi_cast_years[["2008"]]) <- (envi_cast_years[["2008"]][,1])
rownames(envi_cast_years[["2009"]]) <- (envi_cast_years[["2009"]][,1])
rownames(envi_cast_years[["2010"]]) <- (envi_cast_years[["2010"]][,1])
rownames(envi_cast_years[["2011"]]) <- (envi_cast_years[["2011"]][,1])
rownames(envi_cast_years[["2012"]]) <- (envi_cast_years[["2012"]][,1])
rownames(envi_cast_years[["2013"]]) <- (envi_cast_years[["2013"]][,1])
rownames(envi_cast_years[["2014"]]) <- (envi_cast_years[["2014"]][,1])
rownames(envi_cast_years[["2015"]]) <- (envi_cast_years[["2015"]][,1])

# Delete column "site_id" of each row object
envi_cast_years <- lapply(envi_cast_years, function(x) {x["site_id"] <- NULL;x})
```

# Check for collinearity. First, over the years 2005-2012 (years for which enough diatom data is available). Then, check all years seperately.
```{r collinearity-check, echo=TRUE, message=FALSE}
## Check for collinearity between explanatory data
# 
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- abs(cor(x, y, use = "complete.obs"))
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste(prefix, txt, sep = "")
  if (missing(cex.cor)) 
    cex <- 0.8/strwidth(txt)
  
  test <- cor.test(x,y)
  # borrowed from printCoefmat
  Signif <- symnum(test$p.value, corr = FALSE, na = FALSE,
                   cutpoints = c(0, 0.05, 0.1, 1),
                   symbols = c("*", ".", " "))
  
  text(0.5, 0.5, txt, cex = cex * r)
  text(.8, .8, Signif, cex = cex, col = 2)
}
pairs(envi_cast[ ,1:10], 
      lower.panel = panel.smooth, upper.panel = panel.cor)

# Figure margins too large to show all the envi-variables.

# Shows correlation matrix. --> Too large to be displayed completely
cor(envi_cast, use = "complete.obs")

# VIF
sort(vif(envi_cast))

```


```{r load-ec50-data, echo=TRUE, message=FALSE}
drv = dbDriver("PostgreSQL")
con = dbConnect(drv, dbname = DBname, user = DBuser, host = DBhost, port = DBport, password = DBpassword)


ec50 = dbGetQuery(con, "SELECT variable_id, subst_name, n_al_f, ec50f_al_min, ec50f_al_min_taxa, ec50f_al_perc5, ec50f_al_mean
                                FROM phch.phch_ec50
                               ")

dbDisconnect(con)
dbUnloadDriver(drv)
```

Data is very inclomplete.
##AS: We don't need data for every single algae. We can aggregate all Freshwater algae.

